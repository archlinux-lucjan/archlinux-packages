From c8d6892cbd57a9397bde072f0acb5ac38f5186bd Mon Sep 17 00:00:00 2001
From: Emil <emil@etsalapatis.com>
Date: Tue, 10 Jun 2025 11:43:34 -0400
Subject: [PATCH 1/3] lib: remove scx_bpf_error calls and replace with
 bpf_printk

---
 lib/bitmap.bpf.c            |  4 +-
 lib/cpumask.bpf.c           | 10 +++--
 lib/sdt_alloc.bpf.c         | 75 ++++++++++++++++++++-----------------
 lib/topology.bpf.c          | 43 +++++++++++----------
 scheds/include/lib/percpu.h | 18 ++++-----
 5 files changed, 79 insertions(+), 71 deletions(-)

diff --git a/lib/bitmap.bpf.c b/lib/bitmap.bpf.c
index 1807daa8..f8cb4150 100644
--- a/lib/bitmap.bpf.c
+++ b/lib/bitmap.bpf.c
@@ -56,8 +56,8 @@ int scx_bitmap_copy_to_stack(struct scx_bitmap *dst, scx_bitmap_t __arg_arena sr
 	int i;
 
 	if (unlikely(!src || !dst)) {
-		scx_bpf_error("invalid pointer args to pointer copy");
-		return 0;
+		bpf_printk("invalid pointer args to pointer copy");
+		return -EINVAL;
 	}
 
 	bpf_for(i, 0, mask_size) {
diff --git a/lib/cpumask.bpf.c b/lib/cpumask.bpf.c
index e90a6453..c4bcbee3 100644
--- a/lib/cpumask.bpf.c
+++ b/lib/cpumask.bpf.c
@@ -73,12 +73,12 @@ scx_bitmap_vacate_cpu(scx_bitmap_t __arg_arena mask, s32 cpu)
 	int ind = (u32)cpu % 64;
 
 	if (cpu < 0 || cpu >= nr_cpu_ids) {
-		scx_bpf_error("freeing invalid cpu");
+		bpf_printk("freeing invalid cpu");
 		return -EINVAL;
 	}
 
 	if (off < 0 || off >= mask_size || off >= SCXMASK_NLONG) {
-		scx_bpf_error("impossible out-of-bounds on free");
+		bpf_printk("impossible out-of-bounds on free");
 		return -EINVAL;
 	}
 
@@ -98,8 +98,10 @@ scx_bitmap_to_bpf(struct bpf_cpumask __kptr *bpfmask __arg_trusted,
 	scx_bitmap_copy_to_stack(tmp, scx_bitmap);
 
 	ret = __COMPAT_bpf_cpumask_populate((struct cpumask *)bpfmask, tmp->bits, sizeof(tmp->bits));
-	if (unlikely(ret))
-		scx_bpf_error("error %d when calling bpf_cpumask_populate", ret);
+	if (unlikely(ret)) {
+		bpf_printk("error %d when calling bpf_cpumask_populate", ret);
+		return ret;
+	}
 
 	return 0;
 }
diff --git a/lib/sdt_alloc.bpf.c b/lib/sdt_alloc.bpf.c
index 968032ae..4bcf9b92 100644
--- a/lib/sdt_alloc.bpf.c
+++ b/lib/sdt_alloc.bpf.c
@@ -251,12 +251,12 @@ static sdt_desc_t *scx_alloc_chunk(struct scx_alloc_stack __arena *stack)
 static int pool_set_size(struct sdt_pool *pool, __u64 data_size, __u64 nr_pages)
 {
 	if (unlikely(data_size % 8)) {
-		scx_bpf_error("%s: allocation size %llu not word aligned", __func__, data_size);
+		bpf_printk("%s: allocation size %llu not word aligned", __func__, data_size);
 		return -EINVAL;
 	}
 
 	if (unlikely(nr_pages == 0)) {
-		scx_bpf_error("%s: allocation size is 0", __func__);
+		bpf_printk("%s: allocation size is 0", __func__);
 		return -EINVAL;
 	}
 
@@ -384,8 +384,8 @@ int scx_alloc_free_idx(struct scx_allocator *alloc, __u64 idx)
 	desc = alloc->root;
 	if (unlikely(!desc)) {
 		bpf_spin_unlock(&alloc_lock);
-		scx_bpf_error("%s: root not allocated", __func__);
-		return 0;
+		bpf_printk("%s: root not allocated", __func__);
+		return -EINVAL;
 	}
 
 	/* To appease the verifier. */
@@ -411,9 +411,9 @@ int scx_alloc_free_idx(struct scx_allocator *alloc, __u64 idx)
 
 		if (unlikely(!desc)) {
 			bpf_spin_unlock(&alloc_lock);
-			scx_bpf_error("%s: freeing nonexistent idx [0x%llx] (level %llu)",
+			bpf_printk("%s: freeing nonexistent idx [0x%llx] (level %llu)",
 				__func__, idx, level);
-			return 0;
+			return -EINVAL;
 		}
 	}
 
@@ -436,7 +436,7 @@ int scx_alloc_free_idx(struct scx_allocator *alloc, __u64 idx)
 	ret = mark_nodes_avail(lv_desc, lv_pos);
 	if (unlikely(ret != 0)) {
 		bpf_spin_unlock(&alloc_lock);
-		return 0;
+		return ret;
 	}
 
 	alloc_stats.active_allocs -= 1;
@@ -606,7 +606,7 @@ void __arena *scx_static_alloc(size_t bytes, size_t alignment)
 
 	if (alloc_bytes > scx_static.max_alloc_bytes) {
 		bpf_spin_unlock(&alloc_lock);
-		scx_bpf_error("invalid request %ld, max is %ld\n", alloc_bytes,
+		bpf_printk("invalid request %ld, max is %ld\n", alloc_bytes,
 			      scx_static.max_alloc_bytes);
 		return NULL;
 	}
@@ -640,7 +640,7 @@ void __arena *scx_static_alloc(size_t bytes, size_t alignment)
 			bpf_spin_unlock(&alloc_lock);
 			bpf_arena_free_pages(&arena, memory, scx_static.max_alloc_bytes);
 
-			scx_bpf_error("concurrent static memory allocations unsupported");
+			bpf_printk("concurrent static memory allocations unsupported");
 			return NULL;
 		}
 
@@ -699,7 +699,7 @@ int scx_stk_init(struct scx_stk *stack, __u64 data_size, __u64 nr_pages_per_allo
 
 	stack->lock = scx_static_alloc(sizeof(*stack->lock), 1);
 	if (!stack->lock) {
-		scx_bpf_error("failed to allocate lock");
+		bpf_printk("failed to allocate lock");
 		return -ENOMEM;
 	}
 
@@ -836,8 +836,8 @@ int scx_stk_free_internal(struct scx_stk *stack, __u64 elem)
 		return -EINVAL;
 
 	if ((ret = arena_spin_lock(stack->lock))) {
-		scx_bpf_error("spinlock error %d", ret);
-		return 0;
+		bpf_printk("spinlock error %d", ret);
+		return ret;
 	}
 
 	ret = scx_stk_free_unlocked(stack, (void __arena *)elem);
@@ -878,7 +878,7 @@ int scx_stk_get_arena_memory(struct scx_stk *stack, __u64 nr_pages, __u64 nstk_s
 
 	if ((ret = arena_spin_lock(stack->lock))) {
 		bpf_arena_free_pages(&arena, (void __arena *)mem, nr_pages);
-		scx_bpf_error("spinlock error %d", ret);
+		bpf_printk("spinlock error %d", ret);
 		return ret;
 	}
 
@@ -910,7 +910,7 @@ int scx_stk_fill_new_elems(struct scx_stk *stack)
 	nelems = (nr_pages * PAGE_SIZE) / stack->data_size;
 	if (nelems > SCX_STK_SEG_MAX) {
 		arena_spin_unlock(stack->lock);
-		scx_bpf_error("new elements must fit into a single segment");
+		bpf_printk("new elements must fit into a single segment");
 		return -EINVAL;
 	}
 
@@ -974,12 +974,12 @@ __u64 scx_stk_alloc(struct scx_stk *stack)
 	int ret;
 
 	if (!stack) {
-		scx_bpf_error("using uninitialized stack allocator");
+		bpf_printk("using uninitialized stack allocator");
 		return 0ULL;
 	}
 
 	if ((ret = arena_spin_lock(stack->lock))) {
-		scx_bpf_error("spinlock error %d", ret);
+		bpf_printk("spinlock error %d", ret);
 		return 0ULL;
 	}
 
@@ -988,7 +988,7 @@ __u64 scx_stk_alloc(struct scx_stk *stack)
 		/* The call drops the lock on error. */
 		ret = scx_stk_fill_new_elems(stack);
 		if (ret) {
-			scx_bpf_error("elem creation failed");
+			bpf_printk("elem creation failed");
 			return 0ULL;
 		}
 	}
@@ -1000,16 +1000,16 @@ __u64 scx_stk_alloc(struct scx_stk *stack)
 }
 
 static
-void header_set_order(scx_buddy_chunk_t *chunk, u64 offset, u8 order)
+int header_set_order(scx_buddy_chunk_t *chunk, u64 offset, u8 order)
 {
 	if (order >= SCX_BUDDY_CHUNK_MAX_ORDER) {
-		scx_bpf_error("setting invalid order");
-		return;
+		bpf_printk("setting invalid order");
+		return -EINVAL;
 	}
 
 	if (offset >= SCX_BUDDY_CHUNK_ITEMS) {
-		scx_bpf_error("setting order of invalid offset");
-		return;
+		bpf_printk("setting order of invalid offset");
+		return EINVAL;
 	}
 
 	if (offset & 0x1)
@@ -1018,6 +1018,8 @@ void header_set_order(scx_buddy_chunk_t *chunk, u64 offset, u8 order)
 		order <<= 4;
 
 	chunk->orders[offset / 2] |= order;
+
+	return 0;
 }
 
 static
@@ -1028,7 +1030,7 @@ u8 header_get_order(scx_buddy_chunk_t *chunk, u64 offset)
 	_Static_assert(SCX_BUDDY_CHUNK_MAX_ORDER <= 16, "order must fit in 4 bits");
 
 	if (offset >= SCX_BUDDY_CHUNK_ITEMS) {
-		scx_bpf_error("setting order of invalid offset");
+		bpf_printk("setting order of invalid offset");
 		return SCX_BUDDY_CHUNK_MAX_ORDER;
 	}
 
@@ -1044,7 +1046,7 @@ u64 size_to_order(size_t size)
 
 	if (unlikely(!size)) {
 		bpf_printk("size 0 has no order");
-		scx_bpf_error("size 0 has no order");
+		bpf_printk("size 0 has no order");
 		return 64;
 	}
 
@@ -1109,7 +1111,8 @@ scx_buddy_chunk_t *scx_buddy_chunk_get(struct scx_stk *stk)
 		header = chunk_get_header(chunk, i);
 		header->prev_index = SCX_BUDDY_CHUNK_ITEMS;
 		header->next_index = SCX_BUDDY_CHUNK_ITEMS;
-		header_set_order(chunk, i, SCX_BUDDY_CHUNK_MAX_ORDER);
+		if (header_set_order(chunk, i, SCX_BUDDY_CHUNK_MAX_ORDER))
+			return NULL;
 	}
 
 	_Static_assert(SCX_BUDDY_CHUNK_PAGES * PAGE_SIZE >= SCX_BUDDY_MIN_ALLOC_BYTES * SCX_BUDDY_CHUNK_ITEMS,
@@ -1131,7 +1134,6 @@ scx_buddy_chunk_t *scx_buddy_chunk_get(struct scx_stk *stk)
 		power2 = scx_ffs(left);
 		if (unlikely(power2 >= SCX_BUDDY_CHUNK_MAX_ORDER)) {
 			bpf_printk("buddy chunk metadata require allocation of order %d", power2);
-			scx_bpf_error("buddy chunk metadata too large");
 			return NULL;
 		}
 
@@ -1151,7 +1153,8 @@ scx_buddy_chunk_t *scx_buddy_chunk_get(struct scx_stk *stk)
 
 			/* Mark it free. */
 			chunk->order_indices[ord] = idx;
-			header_set_order(chunk, idx, ord);
+			if (header_set_order(chunk, idx, ord))
+				return NULL;
 		}
 
 		/* Adjust the index. */
@@ -1224,7 +1227,8 @@ u64 scx_buddy_chunk_alloc(scx_buddy_chunk_t *chunk, int order_req)
 
 	header->prev_index = SCX_BUDDY_CHUNK_ITEMS;
 	header->next_index = SCX_BUDDY_CHUNK_ITEMS;
-	header_set_order(chunk, idx, order_req);
+	if (header_set_order(chunk, idx, order_req))
+		return (u64)NULL;
 
 	address = (u64)chunk_idx_to_mem(chunk, idx);
 
@@ -1235,7 +1239,8 @@ u64 scx_buddy_chunk_alloc(scx_buddy_chunk_t *chunk, int order_req)
 
 		/* Add the buddy of the allocation to the free list. */
 		header = chunk_get_header(chunk, idx);
-		header_set_order(chunk, idx, order);
+		if (header_set_order(chunk, idx, order))
+			return (u64)NULL;
 		header->prev_index = SCX_BUDDY_CHUNK_ITEMS;
 
 		header->next_index = chunk->order_indices[order];
@@ -1254,7 +1259,7 @@ u64 scx_buddy_alloc_internal(struct scx_buddy *buddy, size_t size)
 
 	order = size_to_order(size);
 	if (order >= SCX_BUDDY_CHUNK_MAX_ORDER - 1) {
-		scx_bpf_error("Allocation size %lu too large", size);
+		bpf_printk("Allocation size %lu too large", size);
 		return (u64)NULL;
 	}
 
@@ -1297,7 +1302,7 @@ void scx_buddy_free_internal(struct scx_buddy *buddy, u64 addr)
 	u8 order;
 
 	if (addr & (SCX_BUDDY_MIN_ALLOC_BYTES - 1)) {
-		scx_bpf_error("Freeing unaligned address %llx", addr);
+		bpf_printk("Freeing unaligned address %llx", addr);
 		return;
 	}
 
@@ -1314,7 +1319,7 @@ void scx_buddy_free_internal(struct scx_buddy *buddy, u64 addr)
 
 	if (chunk == NULL) {
 		bpf_spin_unlock(&buddy->lock);
-		scx_bpf_error("could not find chunk for address %llx", addr);
+		bpf_printk("could not find chunk for address %llx", addr);
 		return;
 	}
 
@@ -1349,12 +1354,14 @@ void scx_buddy_free_internal(struct scx_buddy *buddy, u64 addr)
 			buddy_header->prev_index = SCX_BUDDY_CHUNK_ITEMS;
 		}
 
-		header_set_order(chunk, buddy_idx, SCX_BUDDY_CHUNK_MAX_ORDER);
+		if (header_set_order(chunk, buddy_idx, SCX_BUDDY_CHUNK_MAX_ORDER))
+			return;
 
 		idx = idx < buddy_idx ? idx : buddy_idx;
 
 		header = chunk_get_header(chunk, idx);
-		header_set_order(chunk, idx, order + 1);
+		if (header_set_order(chunk, idx, order + 1))
+			return;
 	}
 
 	order = header_get_order(chunk, idx);
diff --git a/lib/topology.bpf.c b/lib/topology.bpf.c
index 70e7735e..96ac8ecd 100644
--- a/lib/topology.bpf.c
+++ b/lib/topology.bpf.c
@@ -34,7 +34,7 @@ topo_ptr topo_node(topo_ptr parent, scx_bitmap_t mask, u64 id)
 
 	topo = scx_static_alloc(sizeof(struct topology), 1);
 	if (!topo) {
-		scx_bpf_error("static allocation failed");
+		bpf_printk("static allocation failed");
 		return NULL;
 	}
 
@@ -49,12 +49,12 @@ topo_ptr topo_node(topo_ptr parent, scx_bitmap_t mask, u64 id)
 	topo->mask = mask;
 
 	if (topo->level >= TOPO_MAX_LEVEL) {
-		scx_bpf_error("topology is too deep");
+		bpf_printk("topology is too deep");
 		return NULL;
 	}
 
 	if (id >= NR_CPUS) {
-		scx_bpf_error("invalid node id");
+		bpf_printk("invalid node id");
 		return NULL;
 	}
 
@@ -79,7 +79,7 @@ int topo_add(topo_ptr parent, scx_bitmap_t mask, u64 id)
 		return -ENOMEM;
 
 	if (parent->nr_children >= TOPO_MAX_CHILDREN) {
-		scx_bpf_error("topology fanout is too large");
+		bpf_printk("topology fanout is too large");
 		return -EINVAL;
 	}
 
@@ -99,7 +99,7 @@ int topo_init(scx_bitmap_t __arg_arena mask, u64 data_size, u64 id)
 	if (!topo_all) {
 		topo_all = topo_node(NULL, mask, id);
 		if (!topo_all) {
-			scx_bpf_error("couldn't initialize topology");
+			bpf_printk("couldn't initialize topology");
 			return -EINVAL;
 		}
 
@@ -108,7 +108,7 @@ int topo_init(scx_bitmap_t __arg_arena mask, u64 data_size, u64 id)
 
 	for (i = 0; i < TOPO_MAX_LEVEL && can_loop; i++) {
 		if (!topo_subset(topo, mask)) {
-			scx_bpf_error("mask not a subset of a topology node");
+			bpf_printk("mask not a subset of a topology node");
 			topo_print();
 			return -EINVAL;
 		}
@@ -119,7 +119,7 @@ int topo_init(scx_bitmap_t __arg_arena mask, u64 data_size, u64 id)
 				break;
 
 			if (scx_bitmap_intersects(child->mask, mask)) {
-				scx_bpf_error("partially intersecting topology nodes");
+				bpf_printk("partially intersecting topology nodes");
 				return -EINVAL;
 			}
 		}
@@ -134,7 +134,7 @@ int topo_init(scx_bitmap_t __arg_arena mask, u64 data_size, u64 id)
 		}
 
 		if (!child) {
-			scx_bpf_error("child is not valid");
+			bpf_printk("child is not valid");
 			return 0;
 		}
 
@@ -148,7 +148,7 @@ int topo_init(scx_bitmap_t __arg_arena mask, u64 data_size, u64 id)
 			return -ENOMEM;
 	}
 
-	scx_bpf_error("topology is too deep");
+	bpf_printk("topology is too deep");
 	return -EINVAL;
 }
 
@@ -159,7 +159,7 @@ topo_ptr topo_find_descendant(topo_ptr topo, u32 cpu)
 	int lvl, i;
 
 	if (!topo_contains(topo, cpu)) {
-		scx_bpf_error("missing cpu from topology");
+		bpf_printk("missing cpu from topology");
 		return NULL;
 	}
 
@@ -174,7 +174,7 @@ topo_ptr topo_find_descendant(topo_ptr topo, u32 cpu)
 		}
 
 		if (i == topo->nr_children) {
-			scx_bpf_error("missing cpu from inner topology nodes");
+			bpf_printk("missing cpu from inner topology nodes");
 			return NULL;
 		}
 
@@ -191,7 +191,7 @@ topo_ptr topo_find_ancestor(topo_ptr topo, u32 cpu)
 		topo = topo->parent;
 
 	if (!topo_contains(topo, cpu))
-		scx_bpf_error("could not find cpu");
+		bpf_printk("could not find cpu");
 
 	return topo;
 
@@ -205,7 +205,7 @@ topo_ptr topo_find_sibling(topo_ptr topo, u32 cpu)
 	int i;
 
 	if (!parent) {
-		scx_bpf_error("parent has no sibling");
+		bpf_printk("parent has no sibling");
 		return NULL;
 	}
 
@@ -223,12 +223,12 @@ __weak
 u64 topo_mask_level_internal(topo_ptr topo, enum topo_level level)
 {
 	if (unlikely(level < 0 || level >= TOPO_MAX_LEVEL)) {
-		scx_bpf_error("invalid topology level %d", level);
+		bpf_printk("invalid topology level %d", level);
 		return (u64)NULL;
 	}
 
 	if (unlikely(topo->level < level)) {
-		scx_bpf_error("requesting cpumask from lower level %d, starting from %d", level, topo->level);
+		bpf_printk("requesting cpumask from lower level %d, starting from %d", level, topo->level);
 		return (u64)NULL;
 	}
 
@@ -260,13 +260,13 @@ topo_iter_start_from(struct topo_iter *iter, topo_ptr topo)
 		}
 
 		if (ind == parent->nr_children) {
-			scx_bpf_error("could not find topology node in parent");
+			bpf_printk("could not find topology node in parent");
 			return -EINVAL;
 		}
 
 
 		if (unlikely(lvl >= TOPO_MAX_LEVEL)) {
-			scx_bpf_error("invalid level %d", lvl);
+			bpf_printk("invalid level %d", lvl);
 			return -EINVAL;
 		}
 
@@ -291,7 +291,7 @@ topo_iter_next(struct topo_iter *iter)
 	enum topo_level lvl;
 
 	if (unlikely(!iter)) {
-		scx_bpf_error("passing NULL iterator");
+		bpf_printk("passing NULL iterator");
 		return false;
 	}
 
@@ -302,12 +302,12 @@ topo_iter_next(struct topo_iter *iter)
 		lvl = iter->topo->level;
 		if (unlikely(lvl < 0 || lvl >= TOPO_MAX_LEVEL)) {
 			/*
-			 * XXXETSAL: We have both bpf_printk and scx_bpf_error
-			 * out of an abundance of caution: In some cases scx_bpf_error
+			 * XXXETSAL: We have both bpf_printk and bpf_printk
+			 * out of an abundance of caution: In some cases bpf_printk
 			 * does not fire at all, making debugging more difficult.
 			 */
 			bpf_printk("invalid child level %d", lvl);
-			scx_bpf_error("invalid child level %d", lvl);
+			bpf_printk("invalid child level %d", lvl);
 			return false;
 		}
 
@@ -325,7 +325,6 @@ topo_iter_next(struct topo_iter *iter)
 		lvl = iter->topo->level;
 		if (unlikely(lvl < 0 || lvl >= TOPO_MAX_LEVEL)) {
 			bpf_printk("invalid level %d", lvl);
-			scx_bpf_error("invalid level %d", lvl);
 			return false;
 		}
 
diff --git a/scheds/include/lib/percpu.h b/scheds/include/lib/percpu.h
index 7b8fdea4..d2fd98f2 100644
--- a/scheds/include/lib/percpu.h
+++ b/scheds/include/lib/percpu.h
@@ -42,13 +42,13 @@ static s32 create_save_bpfmask(struct bpf_cpumask __kptr **kptr)
 
 	bpfmask = bpf_cpumask_create();
 	if (!bpfmask) {
-		scx_bpf_error("Failed to create bpfmask");
+		bpf_printk("Failed to create bpfmask");
 		return -ENOMEM;
 	}
 
 	bpfmask = bpf_kptr_xchg(kptr, bpfmask);
 	if (bpfmask) {
-		scx_bpf_error("kptr already had cpumask");
+		bpf_printk("kptr already had cpumask");
 		bpf_cpumask_release(bpfmask);
 	}
 
@@ -65,7 +65,7 @@ __weak int scx_storage_init_single(u32 cpu)
 	storage = bpf_map_lookup_percpu_elem(map, &zero, cpu);
 	if (!storage) {
 		/* Should be impossible. */
-		scx_bpf_error("Did not find map entry");
+		bpf_printk("Did not find map entry");
 		return -EINVAL;
 	}
 
@@ -99,12 +99,12 @@ struct bpf_cpumask *scx_percpu_bpfmask(void)
 	storage = bpf_map_lookup_elem(map, &zero);
 	if (!storage) {
 		/* Should be impossible. */
-		scx_bpf_error("Did not find map entry");
+		bpf_printk("Did not find map entry");
 		return NULL;
 	}
 
 	if (!storage->bpfmask)
-		scx_bpf_error("Did not properly initialize singleton bpfmask");
+		bpf_printk("Did not properly initialize singleton bpfmask");
 
 	return storage->bpfmask;
 }
@@ -119,12 +119,12 @@ scx_bitmap_t scx_percpu_scx_bitmap(void)
 	storage = bpf_map_lookup_elem(map, &zero);
 	if (!storage) {
 		/* Should be impossible. */
-		scx_bpf_error("Did not find map entry");
+		bpf_printk("Did not find map entry");
 		return NULL;
 	}
 
 	if (!storage->scx_bitmap)
-		scx_bpf_error("Did not properly initialize singleton scx_bitmap");
+		bpf_printk("Did not properly initialize singleton scx_bitmap");
 
 	return storage->scx_bitmap;
 }
@@ -139,7 +139,7 @@ cpumask_t *scx_percpu_cpumask(void)
 	storage = bpf_map_lookup_elem(map, &zero);
 	if (!storage) {
 		/* Should be impossible. */
-		scx_bpf_error("Did not find map entry");
+		bpf_printk("Did not find map entry");
 		return NULL;
 	}
 
@@ -156,7 +156,7 @@ struct scx_bitmap *scx_percpu_scx_bitmap_stack(void)
 	storage = bpf_map_lookup_elem(map, &zero);
 	if (!storage) {
 		/* Should be impossible. */
-		scx_bpf_error("Did not find map entry");
+		bpf_printk("Did not find map entry");
 		return NULL;
 	}
 
-- 
2.49.0


From ddb34584c32cc4b940b3caa79649d567b0b58959 Mon Sep 17 00:00:00 2001
From: Emil <emil@etsalapatis.com>
Date: Wed, 11 Jun 2025 01:17:31 -0400
Subject: [PATCH 2/3] lib/arena: add more diagnostics on error, including for
 uninitialized nr_cpu_ids

---
 lib/arena.bpf.c             | 17 ++++++++++++++---
 lib/bitmap.bpf.c            |  6 +++---
 scheds/include/lib/arena.h  |  2 ++
 scheds/include/lib/percpu.h |  4 ++--
 4 files changed, 21 insertions(+), 8 deletions(-)

diff --git a/lib/arena.bpf.c b/lib/arena.bpf.c
index fc477f11..77765757 100644
--- a/lib/arena.bpf.c
+++ b/lib/arena.bpf.c
@@ -25,18 +25,29 @@ int arena_init(struct arena_init_args *args)
 	if (ret)
 		return ret;
 
+	if (nr_cpu_ids == NR_CPU_IDS_UNINIT) {
+		bpf_printk("uninitialized nr_cpu_ids variable");
+		return -EINVAL;
+	}
+
 	/* How many types to store all CPU IDs? */
 	ret = scx_bitmap_init(div_round_up(nr_cpu_ids, 8));
-	if (ret)
+	if (ret) {
+		bpf_printk("scx_bitmap_init failed with %d", ret);
 		return ret;
+	}
 
 	ret = scx_percpu_storage_init();
-	if (ret)
+	if (ret) {
+		bpf_printk("scx_percpu_storage_init failed with %d", ret);
 		return ret;
+	}
 
 	ret = scx_task_init(args->task_ctx_size);
-	if (ret)
+	if (ret) {
+		bpf_printk("scx_task_init failed with %d", ret);
 		return ret;
+	}
 
 	return 0;
 }
diff --git a/lib/bitmap.bpf.c b/lib/bitmap.bpf.c
index f8cb4150..84698863 100644
--- a/lib/bitmap.bpf.c
+++ b/lib/bitmap.bpf.c
@@ -1,9 +1,9 @@
 #include <scx/common.bpf.h>
-#include <lib/sdt_task.h>
-
+#include <lib/arena.h>
 #include <lib/cpumask.h>
+#include <lib/sdt_task.h>
 
-const volatile u32 nr_cpu_ids = 64;
+const volatile u32 nr_cpu_ids = NR_CPU_IDS_UNINIT;
 
 static struct scx_allocator scx_bitmap_allocator;
 size_t mask_size;
diff --git a/scheds/include/lib/arena.h b/scheds/include/lib/arena.h
index 2872298f..09635be9 100644
--- a/scheds/include/lib/arena.h
+++ b/scheds/include/lib/arena.h
@@ -1,5 +1,7 @@
 #pragma once
 
+#define NR_CPU_IDS_UNINIT (~(u32)0)
+
 struct arena_init_args {
 	u64 static_pages;
 	u64 task_ctx_size;
diff --git a/scheds/include/lib/percpu.h b/scheds/include/lib/percpu.h
index d2fd98f2..a3b1f8a1 100644
--- a/scheds/include/lib/percpu.h
+++ b/scheds/include/lib/percpu.h
@@ -65,7 +65,7 @@ __weak int scx_storage_init_single(u32 cpu)
 	storage = bpf_map_lookup_percpu_elem(map, &zero, cpu);
 	if (!storage) {
 		/* Should be impossible. */
-		bpf_printk("Did not find map entry");
+		bpf_printk("Did not find map entry for cpu %d", cpu);
 		return -EINVAL;
 	}
 
@@ -119,7 +119,7 @@ scx_bitmap_t scx_percpu_scx_bitmap(void)
 	storage = bpf_map_lookup_elem(map, &zero);
 	if (!storage) {
 		/* Should be impossible. */
-		bpf_printk("Did not find map entry");
+		bpf_printk("Did not find map entry (bitmap)");
 		return NULL;
 	}
 
-- 
2.49.0


From 94257eca4bb136a606c84259c4731e8bb0df2038 Mon Sep 17 00:00:00 2001
From: Emil <emil@etsalapatis.com>
Date: Wed, 11 Jun 2025 01:19:26 -0400
Subject: [PATCH 3/3] scx_chaos: initialize nr_cpu_ids variable during arena
 initialization

---
 scheds/rust/scx_chaos/src/lib.rs | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/scheds/rust/scx_chaos/src/lib.rs b/scheds/rust/scx_chaos/src/lib.rs
index 02945281..011c2482 100644
--- a/scheds/rust/scx_chaos/src/lib.rs
+++ b/scheds/rust/scx_chaos/src/lib.rs
@@ -19,6 +19,7 @@ use scx_utils::uei_report;
 use scx_utils::Core;
 use scx_utils::Llc;
 use scx_utils::Topology;
+use scx_utils::NR_CPU_IDS;
 
 use scx_p2dq::bpf_intf::consts_STATIC_ALLOC_PAGES_GRANULARITY;
 use scx_p2dq::types;
@@ -329,6 +330,7 @@ impl Builder<'_> {
         init_libbpf_logging(None);
 
         let mut open_skel = scx_ops_open!(skel_builder, open_object, chaos)?;
+        open_skel.maps.rodata_data.nr_cpu_ids = *NR_CPU_IDS as u32;
         scx_p2dq::init_open_skel!(&mut open_skel, self.p2dq_opts, self.verbose)?;
 
         // TODO: figure out how to abstract waking a CPU in enqueue properly, but for now disable
-- 
2.49.0

